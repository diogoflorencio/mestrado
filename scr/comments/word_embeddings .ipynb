{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Comment's News 2018\n",
    "\n",
    "The data are comments from news collected in 2018 from the following Brazilian newspapers:\n",
    "\n",
    "* `O Antagonista`, \n",
    "* `O Globo`, \n",
    "* `Veja`.\n",
    "\n",
    "A detailed analysis of the data is available [here](https://pages.github.com/). This notebook's objective is to use the word2vec model to generate embeddings from the texts of these comments. The architecture used by the model is skip-gram, each word is represented by a vector of 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim, logging\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "nltk.download('stopwords')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "PUNCTUATION = u'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ%]' # define news punctuation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining lexicons and loading dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading abbreviation dic\n",
    "with open('../dics/AB_dict') as f:\n",
    "    abbreviation = f.readlines()\n",
    "# process dic    \n",
    "abbreviation = [x.split() for x in abbreviation]\n",
    "abbreviation = {line[0]: line[1] for line in abbreviation}\n",
    "\n",
    "# loading internet_slang dic\n",
    "with open('../dics/IN_dict') as f:\n",
    "    internet_slang = f.readlines()\n",
    "# process dic    \n",
    "internet_slang = [x.split() for x in internet_slang]\n",
    "internet_slang = {line[0]: ' '.join(line[1:]) for line in internet_slang}\n",
    "\n",
    "# loading foreign_word dic\n",
    "with open('../dics/ES_dict') as f:\n",
    "    foreign_word = f.readlines()\n",
    "# process dic    \n",
    "foreign_word = [x.split() for x in foreign_word]\n",
    "foreign_word = [line for line in foreign_word if len(line) > 1] # selecting valid lines\n",
    "foreign_word = {line[0]: ' '.join(line[1:]) for line in foreign_word}\n",
    "\n",
    "# Mapping words in lexicons\n",
    "map_lexicons = {'a ponto':'a_ponto','ao menos ':'ao_menos ','ate mesmo ':'ate_mesmo ',\n",
    "                'nao mais que ':'nao_mais_que ','nem mesmo ':'nem_mesmo ','no minimo ':'no_minimo ',\n",
    "                'o unico ':'o_unico ','a unica ':'a_unica ','pelo menos ':'pelo_menos ',\n",
    "                'quando menos ':'quando_menos ','quando muito ':'quando_muito ','a par disso ':'a_par_disso ',\n",
    "                'e nao ':'e_nao ','em suma ':'em_suma ','mas tambem ': 'mas_tambem ','muito menos ':'muito_menos ',\n",
    "                'nao so ':'nao_so ','ou mesmo ':'ou_mesmo ','por sinal ':'por_sinal ','com isso ':'com_isso ',\n",
    "                'como consequencia ':'como_consequencia ','de modo que ':'de_modo_que ','deste modo ':'deste_modo ',\n",
    "                'em decorrencia ':'em_decorrencia ','nesse sentido ':'nesse_sentido ','por causa ':'por_causa ',\n",
    "                'por conseguinte ':'por_conseguinte ','por essa razao ':'por_essa_razao ','por isso ':'por_isso ',\n",
    "                'sendo assim ':'sendo_assim ','ou entao ':'ou_entao ','ou mesmo ':'ou_mesmo ','como se ':'como_se ',\n",
    "                'de um lado ':'de_um_lado ','por outro lado ':'por_outro_lado ','mais que ':'mais_que ',\n",
    "                'menos que ':'menos_que ','desde que ':'desde_que ','do contrario ':'do_contrario ',\n",
    "                'em lugar ':'em_lugar ','em vez ':'em_vez','no caso ':'no_caso ','se acaso ':'se_acaso ',\n",
    "                'de certa forma ':'de_certa_forma ','desse modo ':'desse_modo ','em funcao ':'em_funcao ',\n",
    "                'isso e ':'isso_e ','ja que ':'ja_que ','na medida que ':'na_medida_que ','nessa direcao ':'nessa_direcao ',\n",
    "                'no intuito ':'no_intuito ','no mesmo sentido ':'no_mesmo_sentido ','ou seja ':'ou_seja ',\n",
    "                'uma vez que ':'uma_vez_que ','tanto que ':'tanto_que ','visto que ':'visto_que ','ainda que ':'ainda_que ',\n",
    "                'ao contrario ':'ao_contrario ','apesar de ':'apesar_de ','fora isso ':'fora_isso ','mesmo que ':'mesmo_que ',\n",
    "                'nao obstante ':'nao_obstante ','nao fosse isso ':'nao_fosse_isso ','no entanto ':'no_entanto ',\n",
    "                'para tanto ':'para_tanto ','pelo contrario ':'pelo_contrario ','por sua vez ':'por_sua_vez ','posto que ':'posto_que '\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word from text into lexicons\n",
    "def word2lexicon(text):\n",
    "    for k, v in map_lexicons.items():\n",
    "        text = str(text).replace(k,v)\n",
    "    return text\n",
    "\n",
    "# Lexical normalization\n",
    "def lexical_normalization(text):\n",
    "    for k, v in abbreviation.items():\n",
    "        text = str(text).replace(k,v)        \n",
    "    for k, v in internet_slang.items():\n",
    "        text = str(text).replace(k,v)        \n",
    "    for k, v in foreign_word.items():\n",
    "        text = str(text).replace(k,v)\n",
    "    return text\n",
    "\n",
    "# function for processing sentences\n",
    "def process_sentences(text):\n",
    "    stop_words = stopwords.words('portuguese') # load stop words\n",
    "    text = re.sub(PUNCTUATION, ' ', str(text)) # remove punctuation from text\n",
    "    text = str(text).split() # split sentences by words\n",
    "    text = [word for word in text if word not in stop_words] # Remove stopwords\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Comentários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST_IP = '192.168.1.7'\n",
    "# init mongo client\n",
    "client = MongoClient(HOST_IP, 27017)\n",
    "\n",
    "#select db\n",
    "db = client['news_2018']\n",
    "\n",
    "# load data\n",
    "oantagonistaComments = pd.DataFrame(list(db.get_collection('oantagonistaComments').find()))\n",
    "ogloboComments = pd.DataFrame(list(db.get_collection('ogloboComments').find()))\n",
    "vejaComments = pd.DataFrame(list(db.get_collection('vejaComments').find()))\n",
    "\n",
    "# concat all comments\n",
    "comments = pd.concat((oantagonistaComments, ogloboComments, vejaComments), sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Comments Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing comments text\n",
    "comments['text'] = comments['text'].apply(lexical_normalization)\n",
    "comments['text'] = comments['text'].apply(word2lexicon)  \n",
    "comments['text'] = comments['text'].apply(process_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec model - settings: approach skip-gram, size embeddings vectors 300 \n",
    "model = gensim.models.Word2Vec(comments['text'], workers=4, size=300, sg=1, window=5, min_count=5)\n",
    "# Saving model\n",
    "model.save('../embeddings/comments_w2v.bin')\n",
    "# Saving embeddings\n",
    "model.wv.save_word2vec_format(\"../embeddings/comments_vectors.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=[u'pontes'], negative=[u'presidente']))\n",
    "\n",
    "# loading embeddings\n",
    "from gensim.models import Word2Vec\n",
    "new_model = Word2Vec.load('../embeddings/comments_w2v.bin')\n",
    "print(new_model.wv.most_similar(positive=[u'pontes'], negative=[u'presidente']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
