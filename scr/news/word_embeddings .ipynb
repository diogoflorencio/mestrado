{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings from News\n",
    "\n",
    "The data are news from 2018, collected from the main media outlets in Brazil::\n",
    "* `Carta Capital`, \n",
    "* `El Pais`,\n",
    "* `Estadao`, \n",
    "* `Folha de São Paulo`, \n",
    "*  `Gazeta do Povo`,\n",
    "* `O Antagonista`, \n",
    "* `O Globo`, \n",
    "* `Veja`\n",
    "\n",
    "A detailed analysis of the data is available [here](https://pages.github.com/). This notebook's objective is to use the word2vec model to generate embeddings from the texts of this news. The architecture used by the model is skip-gram, where each word is represented by a vector of 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules and setting log format\n",
    "import re\n",
    "import nltk\n",
    "import gensim, logging\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "nltk.download('stopwords')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "PUNCTUATION = u'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ%]' # define news punctuation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Lexicons and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping words in lexicons\n",
    "map_lexicons = {'a ponto':'a_ponto','ao menos ':'ao_menos ','ate mesmo ':'ate_mesmo ',\n",
    "                'nao mais que ':'nao_mais_que ','nem mesmo ':'nem_mesmo ','no minimo ':'no_minimo ',\n",
    "                'o unico ':'o_unico ','a unica ':'a_unica ','pelo menos ':'pelo_menos ',\n",
    "                'quando menos ':'quando_menos ','quando muito ':'quando_muito ','a par disso ':'a_par_disso ',\n",
    "                'e nao ':'e_nao ','em suma ':'em_suma ','mas tambem ': 'mas_tambem ','muito menos ':'muito_menos ',\n",
    "                'nao so ':'nao_so ','ou mesmo ':'ou_mesmo ','por sinal ':'por_sinal ','com isso ':'com_isso ',\n",
    "                'como consequencia ':'como_consequencia ','de modo que ':'de_modo_que ','deste modo ':'deste_modo ',\n",
    "                'em decorrencia ':'em_decorrencia ','nesse sentido ':'nesse_sentido ','por causa ':'por_causa ',\n",
    "                'por conseguinte ':'por_conseguinte ','por essa razao ':'por_essa_razao ','por isso ':'por_isso ',\n",
    "                'sendo assim ':'sendo_assim ','ou entao ':'ou_entao ','ou mesmo ':'ou_mesmo ','como se ':'como_se ',\n",
    "                'de um lado ':'de_um_lado ','por outro lado ':'por_outro_lado ','mais que ':'mais_que ',\n",
    "                'menos que ':'menos_que ','desde que ':'desde_que ','do contrario ':'do_contrario ',\n",
    "                'em lugar ':'em_lugar ','em vez ':'em_vez','no caso ':'no_caso ','se acaso ':'se_acaso ',\n",
    "                'de certa forma ':'de_certa_forma ','desse modo ':'desse_modo ','em funcao ':'em_funcao ',\n",
    "                'isso e ':'isso_e ','ja que ':'ja_que ','na medida que ':'na_medida_que ','nessa direcao ':'nessa_direcao ',\n",
    "                'no intuito ':'no_intuito ','no mesmo sentido ':'no_mesmo_sentido ','ou seja ':'ou_seja ',\n",
    "                'uma vez que ':'uma_vez_que ','tanto que ':'tanto_que ','visto que ':'visto_que ','ainda que ':'ainda_que ',\n",
    "                'ao contrario ':'ao_contrario ','apesar de ':'apesar_de ','fora isso ':'fora_isso ','mesmo que ':'mesmo_que ',\n",
    "                'nao obstante ':'nao_obstante ','nao fosse isso ':'nao_fosse_isso ','no entanto ':'no_entanto ',\n",
    "                'para tanto ':'para_tanto ','pelo contrario ':'pelo_contrario ','por sua vez ':'por_sua_vez ','posto que ':'posto_que '\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2lexicon(text):\n",
    "    for k, v in map_lexicons.items():\n",
    "        text = str(text.lower()).replace(k,v)\n",
    "    return text\n",
    "\n",
    "def processSentences(text):\n",
    "    stop_words = stopwords.words('portuguese') # load stop words\n",
    "    text = re.sub(PUNCTUATION, ' ', str(text)) # remove punctuation from text\n",
    "    text = str(text).split() # split sentences by words\n",
    "    text = [word for word in text if word not in stop_words] # Remove stopwords\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST_IP = '192.168.1.7'\n",
    "\n",
    "# init mongo client\n",
    "client = MongoClient(HOST_IP, 27017)\n",
    "#select db\n",
    "db = client['news_2018']\n",
    "\n",
    "# load data\n",
    "carta_capital = pd.DataFrame(list(db.get_collection('carta_capital').find()))\n",
    "el_pais = pd.DataFrame(list(db.get_collection('el_pais').find()))\n",
    "estadao = pd.DataFrame(list(db.get_collection('estadao').find()))\n",
    "folha = pd.DataFrame(list(db.get_collection('folha').find()))\n",
    "gazeta_do_povo = pd.DataFrame(list(db.get_collection('gazeta_do_povo').find()))\n",
    "oantagonista = pd.DataFrame(list(db.get_collection('oantagonista').find()))\n",
    "oglobo = pd.DataFrame(list(db.get_collection('oglobo').find()))\n",
    "veja = pd.DataFrame(list(db.get_collection('veja').find()))\n",
    "\n",
    "# concat all news\n",
    "news = pd.concat((carta_capital, el_pais, estadao, folha, gazeta_do_povo, oantagonista, oglobo, veja), sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing news text\n",
    "news['text'] = news['text'].apply(word2lexicon) \n",
    "news['text'] = news['text'].apply(processSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec model - settings: approach skip-gram, size embeddings vectors 300 \n",
    "model = gensim.models.Word2Vec(news['text'], workers=4, size=300, sg=1, window=5, min_count=5)\n",
    "# Saving model\n",
    "model.save('../embeddings/news_w2v.bin')\n",
    "# Saving embeddings\n",
    "model.wv.save_word2vec_format(\"../embeddings/news_vectors.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=[u'pontes'], negative=[u'presidente']))\n",
    "\n",
    "# Loading embeddings\n",
    "from gensim.models import Word2Vec\n",
    "new_model = Word2Vec.load('../embeddings/news_w2v.bin')\n",
    "print(new_model.wv.most_similar(positive=[u'pontes'], negative=[u'presidente']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
