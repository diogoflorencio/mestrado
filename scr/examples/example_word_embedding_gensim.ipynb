{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa as bibliotecas\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os dados de treino. Note que as sentencas ja entao separadas em tokens.\n",
    "sentences = [\n",
    "    ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina o modelo. Note que estamos setando o parametro sd = 0, ou seja, o algoritmo utilizado sera o CBOW\n",
    "# Outro ponto de destaque é que quando setamos o parametro min_count para 1 estamos forcando o uso de todas as palavras.\n",
    "\n",
    "model = Word2Vec(sentences, size=100, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Exibe os parametros do meodelo treinado\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n"
     ]
    }
   ],
   "source": [
    "# Exibe o vocabulario do modelo treinado\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6470940e-04  4.9301521e-03  7.5455179e-04 -5.3011288e-04\n",
      " -2.4613196e-03  2.9665576e-03 -1.7253578e-03 -9.5032970e-04\n",
      "  2.0618308e-03  1.9421929e-03  3.3472003e-03  1.8789346e-03\n",
      "  3.5447346e-03 -7.6705479e-04  1.3845407e-03  1.8027367e-03\n",
      "  1.2348316e-03 -4.6784850e-03 -4.7376682e-03  3.0694252e-03\n",
      "  4.2866077e-03  3.1496620e-05 -2.0588622e-03 -4.3346505e-03\n",
      "  3.1636469e-03 -2.7192526e-03  2.6917290e-03 -2.7376388e-03\n",
      " -2.1027839e-03 -3.6995877e-03 -3.6955867e-03  3.2027447e-04\n",
      "  3.3230826e-03  3.4943535e-03 -4.9421629e-03  4.9811197e-03\n",
      "  3.8277148e-03  3.1060232e-03  2.8369261e-03 -4.8165033e-03\n",
      " -1.8343912e-03  2.1972337e-03  1.2570077e-03  2.3124251e-03\n",
      " -3.6973711e-03  9.2852925e-04  1.0617559e-03  2.1654745e-03\n",
      " -2.4344116e-03  3.0411617e-04 -3.6191158e-03 -6.8049558e-04\n",
      "  2.4296192e-03 -5.4453599e-04  9.1978320e-04 -8.5888657e-04\n",
      "  1.6755227e-03 -1.1106315e-03 -5.6149915e-04  2.4366722e-04\n",
      " -2.3129880e-03 -4.6850820e-03  3.5197032e-03 -4.3059713e-03\n",
      "  2.1993883e-03  3.1964600e-03  7.1559298e-05 -4.6006450e-03\n",
      "  3.4668362e-03 -1.5293392e-03  1.6115267e-03 -1.4962179e-03\n",
      "  4.9952953e-03 -1.0104472e-03  3.7933576e-03  4.4299592e-03\n",
      " -4.9202675e-03  1.6274618e-03 -3.0432572e-03  3.0021083e-03\n",
      " -6.1280053e-04 -4.7169095e-03 -9.2219940e-04 -2.2241469e-03\n",
      "  3.7270610e-03 -9.5533446e-04 -1.5773836e-03  1.2289541e-03\n",
      " -3.1021130e-04  3.9769830e-03 -4.9981368e-03  3.1334262e-03\n",
      " -2.9565762e-03 -4.0008370e-03 -2.2820249e-04  3.3652436e-03\n",
      "  2.5816492e-04 -1.4613097e-03  3.3959562e-03  4.5452155e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diogo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Acesso o vetor da palavra \"sentence\"\n",
    "print(model['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salva o modelo\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Carrega o modelo\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weslley\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# Nesta etapa vamos usar PCA (Principal Component Analysis) para visualizar o modelo embedding que criamos.\n",
    "# Selecionares os dois principais componentes da PCA e os mosraremos em um scatter plot.\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Como dito, não necessariamente precisamos criar o nosso próprio embedding word. Podemos utilizar um modelo já treinado.\n",
    "# Neste exemplo vamos usar o modelo disponibilizado pelo Google, treinado no dataset do Google News que originalmente possui\n",
    "# em torno de 100 bilhões de palavras. O modelo final possui 3 milhões de palavras representadas em um vetor de 300 dimensões.\n",
    "\n",
    "# Fazer download do dataset em https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "# load the google word2vec model\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "environment": null,
   "summary": "Word embedding com Gensim"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
